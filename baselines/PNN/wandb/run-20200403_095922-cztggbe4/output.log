Loading a model without an environment, this model cannot be trained until it has a valid environment.
output/gridworld_continuous/MO_RR2LR_ppn
/iliad/u/caozj/anaconda3/envs/reward/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
---------------------------------------
| approxkl           | 0.00012715576  |
| clipfrac           | 0.0            |
| ep_len_mean        | 78.5           |
| ep_reward_mean     | -145           |
| explained_variance | -0.000104      |
| fps                | 306            |
| n_updates          | 1              |
| policy_entropy     | 1.4216294      |
| policy_loss        | -0.00023331749 |
| serial_timesteps   | 128            |
| time_elapsed       | 1.91e-06       |
| total_timesteps    | 128            |
| value_loss         | 5926815.5      |
---------------------------------------
total mean ep return:  -13688.025220890895 [-13688.025220890895]
nsteps:  124
Saving new best model
--------------------------------------
| approxkl           | 0.0035483346  |
| clipfrac           | 0.04296875    |
| ep_len_mean        | 61.5          |
| ep_reward_mean     | -222          |
| explained_variance | 0.000124      |
| fps                | 790           |
| n_updates          | 2             |
| policy_entropy     | 1.4301518     |
| policy_loss        | -0.0049855206 |
| serial_timesteps   | 256           |
| time_elapsed       | 0.604         |
| total_timesteps    | 256           |
| value_loss         | 7312814.0     |
--------------------------------------
total mean ep return:  -2411220.9119117726 [-2411220.9119117726]
nsteps:  49
-------------------------------------
| approxkl           | 0.0005756559 |
| clipfrac           | 0.0          |
| ep_len_mean        | 59.3         |
| ep_reward_mean     | 10.6         |
| explained_variance | -0.000406    |
| fps                | 802          |
| n_updates          | 3            |
| policy_entropy     | 1.4295108    |
| policy_loss        | 0.0020597777 |
| serial_timesteps   | 384          |
| time_elapsed       | 0.824        |
| total_timesteps    | 384          |
| value_loss         | 190180.16    |
-------------------------------------
total mean ep return:  -2309222.389403737 [-2309222.389403737]
nsteps:  45
---------------------------------------
| approxkl           | 7.281804e-05   |
| clipfrac           | 0.0            |
| ep_len_mean        | 101            |
| ep_reward_mean     | 3.48           |
| explained_variance | -2.28e-05      |
| fps                | 801            |
| n_updates          | 4              |
| policy_entropy     | 1.4282707      |
| policy_loss        | -3.0504074e-05 |
| serial_timesteps   | 512            |
| time_elapsed       | 1.04           |
| total_timesteps    | 512            |
| value_loss         | 4981059.5      |
---------------------------------------
total mean ep return:  -2309222.5774455215 [-2309222.5774455215]
nsteps:  45
---------------------------------------
| approxkl           | 5.2677904e-05  |
| clipfrac           | 0.0            |
| ep_len_mean        | 47.6           |
| ep_reward_mean     | -253           |
| explained_variance | 0.000168       |
| fps                | 813            |
| n_updates          | 5              |
| policy_entropy     | 1.427975       |
| policy_loss        | -0.00050846406 |
| serial_timesteps   | 640            |
| time_elapsed       | 1.26           |
| total_timesteps    | 640            |
| value_loss         | 13845352.0     |
---------------------------------------
total mean ep return:  -2210201.572363685 [-2210201.572363685]
nsteps:  44
--------------------------------------
| approxkl           | 0.00046655047 |
| clipfrac           | 0.0           |
| ep_len_mean        | 50.9          |
| ep_reward_mean     | -133          |
| explained_variance | 0.000307      |
| fps                | 805           |
| n_updates          | 6             |
| policy_entropy     | 1.4284911     |
| policy_loss        | 0.00098584    |
| serial_timesteps   | 768           |
| time_elapsed       | 1.47          |
| total_timesteps    | 768           |
| value_loss         | 5893602.0     |
--------------------------------------
total mean ep return:  -2010137.7723754512 [-2010137.7723754512]
nsteps:  43
--------------------------------------
| approxkl           | 1.5143238e-05 |
| clipfrac           | 0.0           |
| ep_len_mean        | 52            |
| ep_reward_mean     | -144          |
| explained_variance | 0.000488      |
| fps                | 793           |
| n_updates          | 7             |
| policy_entropy     | 1.4287114     |
| policy_loss        | -0.0002109789 |
| serial_timesteps   | 896           |
| time_elapsed       | 1.68          |
| total_timesteps    | 896           |
| value_loss         | 5520353.0     |
--------------------------------------
total mean ep return:  -1608083.474682874 [-1608083.474682874]
nsteps:  42
--------------------------------------
| approxkl           | 0.0042244145  |
| clipfrac           | 0.041015625   |
| ep_len_mean        | 93.4          |
| ep_reward_mean     | -57.8         |
| explained_variance | 0.00142       |
| fps                | 762           |
| n_updates          | 8             |
| policy_entropy     | 1.4271405     |
| policy_loss        | -0.0042351657 |
| serial_timesteps   | 1024          |
| time_elapsed       | 1.89          |
| total_timesteps    | 1024          |
| value_loss         | 1165893.9     |
--------------------------------------
total mean ep return:  -2309222.9259803505 [-2309222.9259803505]
nsteps:  45
------------------------------------
| approxkl           | 0.003584709 |
| clipfrac           | 0.037109375 |
| ep_len_mean        | 58.6        |
| ep_reward_mean     | -1.18e+03   |
| explained_variance | -8.48e-05   |
| fps                | 819         |
| n_updates          | 9           |
| policy_entropy     | 1.4341083   |
| policy_loss        | 0.008460181 |
| serial_timesteps   | 1152        |
| time_elapsed       | 2.12        |
| total_timesteps    | 1152        |
| value_loss         | 386499170.0 |
------------------------------------
total mean ep return:  -2309222.9259803505 [-2309222.9259803505]
nsteps:  45
--------------------------------------
| approxkl           | 0.00083888974 |
| clipfrac           | 0.0           |
| ep_len_mean        | 35.3          |
| ep_reward_mean     | -186          |
| explained_variance | 3.32e-05      |
| fps                | 820           |
| n_updates          | 10            |
| policy_entropy     | 1.4398013     |
| policy_loss        | -0.0020172526 |
| serial_timesteps   | 1280          |
| time_elapsed       | 2.33          |
| total_timesteps    | 1280          |
| value_loss         | 9363407.0     |
--------------------------------------
total mean ep return:  -2309222.9259803505 [-2309222.9259803505]
nsteps:  45
--------------------------------------
| approxkl           | 0.00034125475 |
| clipfrac           | 0.0           |
| ep_len_mean        | 36.2          |
| ep_reward_mean     | -6.2e+03      |
| explained_variance | 8.94e-07      |
| fps                | 827           |
| n_updates          | 11            |
| policy_entropy     | 1.4416443     |
| policy_loss        | -0.0049565276 |
| serial_timesteps   | 1408          |
| time_elapsed       | 2.54          |
| total_timesteps    | 1408          |
| value_loss         | 11169116000.0 |
--------------------------------------
total mean ep return:  -2309222.9259803505 [-2309222.9259803505]
nsteps:  45
--------------------------------------
| approxkl           | 0.008201292   |
| clipfrac           | 0.087890625   |
| ep_len_mean        | 26.4          |
| ep_reward_mean     | -8.33e+03     |
| explained_variance | 4.17e-06      |
| fps                | 826           |
| n_updates          | 12            |
| policy_entropy     | 1.4423065     |
| policy_loss        | -0.018627953  |
| serial_timesteps   | 1536          |
| time_elapsed       | 2.75          |
| total_timesteps    | 1536          |
| value_loss         | 12487401000.0 |
--------------------------------------
total mean ep return:  -906047.651013414 [-906047.651013414]
nsteps:  39
--------------------------------------
| approxkl           | 0.002428486   |
| clipfrac           | 0.02734375    |
| ep_len_mean        | 58.5          |
| ep_reward_mean     | -270          |
| explained_variance | 4.62e-05      |
| fps                | 828           |
| n_updates          | 13            |
| policy_entropy     | 1.4429802     |
| policy_loss        | -0.0048931297 |
| serial_timesteps   | 1664          |
| time_elapsed       | 2.95          |
| total_timesteps    | 1664          |
| value_loss         | 12485334.0    |
--------------------------------------
total mean ep return:  -906051.0119028816 [-906051.0119028816]
nsteps:  39
-------------------------------------
| approxkl           | 0.0022687325 |
| clipfrac           | 0.021484375  |
| ep_len_mean        | 53.9         |
| ep_reward_mean     | -293         |
| explained_variance | 0.000179     |
| fps                | 805          |
| n_updates          | 14           |
| policy_entropy     | 1.4371984    |
| policy_loss        | 0.012912516  |
| serial_timesteps   | 1792         |
| time_elapsed       | 3.16         |
| total_timesteps    | 1792         |
| value_loss         | 12068448.0   |
-------------------------------------
total mean ep return:  -16992.046195046496 [-16992.046195046496]
nsteps:  42
--------------------------------------
| approxkl           | 0.00013516322 |
| clipfrac           | 0.0           |
| ep_len_mean        | 66.9          |
| ep_reward_mean     | -244          |
| explained_variance | -3.47e-05     |
| fps                | 825           |
| n_updates          | 15            |
| policy_entropy     | 1.4348402     |
| policy_loss        | -0.0009908518 |
| serial_timesteps   | 1920          |
| time_elapsed       | 3.37          |
| total_timesteps    | 1920          |
| value_loss         | 8530312.0     |
--------------------------------------
total mean ep return:  -6015.122191089506 [-6015.122191089506]
nsteps:  40
Saving new best model
--------------------------------------
| approxkl           | 0.0022447165  |
| clipfrac           | 0.033203125   |
| ep_len_mean        | 44.2          |
| ep_reward_mean     | -336          |
| explained_variance | 0.000149      |
| fps                | 820           |
| n_updates          | 16            |
| policy_entropy     | 1.4346778     |
| policy_loss        | -0.0038648476 |
| serial_timesteps   | 2048          |
| time_elapsed       | 3.58          |
| total_timesteps    | 2048          |
| value_loss         | 17829846.0    |
--------------------------------------
total mean ep return:  -17826.759780127046 [-17826.759780127046]
nsteps:  51
------------------------------------
| approxkl           | 0.002251    |
| clipfrac           | 0.03125     |
| ep_len_mean        | 39.7        |
| ep_reward_mean     | -272        |
| explained_variance | 0.000206    |
| fps                | 807         |
| n_updates          | 17          |
| policy_entropy     | 1.4391801   |
| policy_loss        | 0.008935526 |
| serial_timesteps   | 2176        |
| time_elapsed       | 3.8         |
| total_timesteps    | 2176        |
| value_loss         | 8630900.0   |
------------------------------------
total mean ep return:  -17793.51938762454 [-17793.51938762454]
nsteps:  52
-------------------------------------
| approxkl           | 0.0024889542 |
| clipfrac           | 0.0078125    |
| ep_len_mean        | 45.1         |
| ep_reward_mean     | -275         |
| explained_variance | 0.000129     |
| fps                | 838          |
| n_updates          | 18           |
| policy_entropy     | 1.4409457    |
| policy_loss        | 0.0051927655 |
| serial_timesteps   | 2304         |
| time_elapsed       | 4.02         |
| total_timesteps    | 2304         |
| value_loss         | 15472611.0   |
-------------------------------------
total mean ep return:  -17791.29893876435 [-17791.29893876435]
nsteps:  52
------------------------------------
| approxkl           | 0.02808033  |
| clipfrac           | 0.2890625   |
| ep_len_mean        | 41.2        |
| ep_reward_mean     | -321        |
| explained_variance | 6.66e-05    |
| fps                | 829         |
| n_updates          | 19          |
| policy_entropy     | 1.4411252   |
| policy_loss        | 0.035434246 |
| serial_timesteps   | 2432        |
| time_elapsed       | 4.23        |
| total_timesteps    | 2432        |
| value_loss         | 13163446.0  |
------------------------------------
total mean ep return:  -17943.1747076124 [-17943.1747076124]
nsteps:  45
--------------------------------------
| approxkl           | 0.0005197389  |
| clipfrac           | 0.0           |
| ep_len_mean        | 33.5          |
| ep_reward_mean     | -217          |
| explained_variance | 0.000129      |
| fps                | 826           |
| n_updates          | 20            |
| policy_entropy     | 1.4412177     |
| policy_loss        | -0.0021468615 |
| serial_timesteps   | 2560          |
| time_elapsed       | 4.44          |
| total_timesteps    | 2560          |
| value_loss         | 11446094.0    |
--------------------------------------
total mean ep return:  -17926.98286166948 [-17926.98286166948]
nsteps:  46
--------------------------------------
| approxkl           | 0.00012616937 |
| clipfrac           | 0.0           |
| ep_len_mean        | 34.1          |
| ep_reward_mean     | -256          |
| explained_variance | 0.000155      |
| fps                | 814           |
| n_updates          | 21            |
| policy_entropy     | 1.4411213     |
| policy_loss        | 0.00131993    |
| serial_timesteps   | 2688          |
| time_elapsed       | 4.65          |
| total_timesteps    | 2688          |
| value_loss         | 14271525.0    |
--------------------------------------
total mean ep return:  -16936.997070515667 [-16936.997070515667]
nsteps:  45
--------------------------------------
| approxkl           | 0.012652425   |
| clipfrac           | 0.140625      |
| ep_len_mean        | 42.3          |
| ep_reward_mean     | -336          |
| explained_variance | 0.000156      |
| fps                | 814           |
| n_updates          | 22            |
| policy_entropy     | 1.4400817     |
| policy_loss        | -0.0032728652 |
| serial_timesteps   | 2816          |
| time_elapsed       | 4.86          |
| total_timesteps    | 2816          |
| value_loss         | 17132182.0    |
--------------------------------------
total mean ep return:  -17857.234331830372 [-17857.234331830372]
nsteps:  49
